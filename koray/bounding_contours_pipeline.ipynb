{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f72fcd-4caf-4b9c-8de0-45da9d2e30a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Contours Visualization Pipeline\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f3c94d6-f27c-4767-b685-89dd74f2d665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless in /srv/conda/envs/notebook/lib/python3.8/site-packages (4.5.4.58)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from opencv-python-headless) (1.20.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "184222ae-d6bd-4b08-a9fc-1ed66522870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import fsspec\n",
    "\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams['figure.figsize'] = 12,8\n",
    "\n",
    "import getpass\n",
    "import azure.storage.blob\n",
    "from azure.storage.blob import BlobClient, BlobServiceClient\n",
    "from azure.core.exceptions import ResourceExistsError, HttpResponseError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7126788b-269b-47fa-b325-aca1e025e0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ···········································································································································\n"
     ]
    }
   ],
   "source": [
    "SAS_TOKEN = getpass.getpass() # of the whole \"cmip6\" folder in Azure.\n",
    "URL_PREFIX = 'https://nasanex30analysis.blob.core.windows.net/cmip6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5556324d-ecb3-4b34-86a0-0ed07b5f05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# CONSTANTS\n",
    "####################################\n",
    "\n",
    "# constants for openCV countour finding\n",
    "SMOOTH_RATIO = 0\n",
    "MIN_AREA = 10\n",
    "CONVEX = False\n",
    "\n",
    "# constants for the rolling-window aggregation\n",
    "ROLLING = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "306757f4-4af7-49a6-9015-de155eb61162",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Utils\n",
    "####################################\n",
    "\n",
    "class AzureSource():\n",
    "\n",
    "    def __init__(self, model:str, year:int):\n",
    "        fn = f\"Ext_max_t__Rgn_1__{year}__Abv_Avg_5_K_for_3_days__CMIP6_{model}_Avg_yrs_1950_79.nc\"\n",
    "        self.filename = fn\n",
    "        abspath = f\"extremes_max/{model}/Region_1/Avg_yrs_1950_79/Abv_Avg_5_K_for_3_days/{fn}\" \n",
    "        self.abspath = abspath\n",
    "            \n",
    "    def download(self):\n",
    "        \n",
    "        if not os.path.isfile(self.filename):\n",
    "            \n",
    "            sas_url = f\"{URL_PREFIX}/{self.abspath}?{SAS_TOKEN}\"\n",
    "            blob_client = BlobClient.from_blob_url(sas_url)\n",
    "\n",
    "            with tempfile.TemporaryFile() as f:\n",
    "                fp = f\"{f.name}.tmp\"\n",
    "                with open(fp, \"wb\") as my_blob:\n",
    "                    download_stream = blob_client.download_blob()\n",
    "                    my_blob.write(download_stream.readall())\n",
    "\n",
    "                    os.rename(fp, self.filename)\n",
    "                    while os.path.getsize(self.filename)/10**6 < 10: # MB\n",
    "                        time.sleep(2) \n",
    "class AzureTarget():\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        \n",
    "    def upload(self, upload_folder:str):\n",
    "        \n",
    "        sas_url = f\"{URL_PREFIX}/{upload_folder}/{self.filename}?{SAS_TOKEN}\"\n",
    "        blob_client = BlobClient.from_blob_url(sas_url)\n",
    "        \n",
    "        with open(self.filename, \"rb\") as f:\n",
    "            if blob_client.exists():\n",
    "                warnings.warn(f\"{self.filename} exists. Overwriting..\")\n",
    "            blob_client.upload_blob(f, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e03a1820-2edb-4d05-b7b4-4e300f918ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Define Contour obj\n",
    "####################################\n",
    "\n",
    "\"\"\"\n",
    "Bounding-contours algorithm to find the extend of the heat events and\n",
    "produce visualizations. It uses the the heat events y/n dataset \n",
    "which was (supposed to be pre-) produced by the \"Heatwave Analysis\" algorithm. \n",
    "\"\"\"\n",
    "\n",
    "class Contour(object):\n",
    "    \"\"\"A single contour obj. All unit operations are managed here.\"\"\"\n",
    "    \n",
    "    def __init__(self, cnt:np.array, lons, lats):\n",
    "        self.contour = cnt\n",
    "        self.lons = lons\n",
    "        self.lats = lats\n",
    "        self.name = uuid.uuid4().hex[:6]\n",
    "        self._area = 0.0\n",
    "        self._smoothened = np.array([], dtype=np.int32)\n",
    "        self._projected = np.array([], dtype=np.float64)\n",
    "        self._center = ()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "    \n",
    "    @property\n",
    "    def area(self):\n",
    "        return cv2.contourArea(self.contour)\n",
    "    \n",
    "    @property\n",
    "    def smoothened(self):\n",
    "        cnt = self.contour\n",
    "        arc = SMOOTH_RATIO*cv2.arcLength(cnt,True)\n",
    "        return cv2.approxPolyDP(cnt,arc,True)\n",
    "    \n",
    "    @property\n",
    "    def projected(self):\n",
    "        squeezed = self.smoothened.squeeze()\n",
    "        proj = [(float(self.lons[x]), float(self.lats[y])) for (x,y) in squeezed]\n",
    "        return np.array(proj).reshape((-1,1,2))\n",
    "    \n",
    "    @property\n",
    "    def center(self):\n",
    "        M = cv2.moments(self.contour)\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "        return (float(self.lons[cX]), float(self.lats[cY])) \n",
    "\n",
    "    def position_to(self, c2:object)->str:\n",
    "        \"\"\"Find the relative position of a Contour obj to another.\n",
    "        Return if c1 is inside or outside c2, or they intersect.\"\"\"\n",
    "        \n",
    "        f = cv2.pointPolygonTest\n",
    "        c1 = self.contour.squeeze().astype(float)\n",
    "        tf = np.array([int((f(c2.contour, x, False))) for x in c1])\n",
    "        if all(tf==-1):\n",
    "            return \"outside\"  \n",
    "        elif all(tf==1):\n",
    "            return \"inside\" \n",
    "        else:\n",
    "            return \"intersect\" \n",
    "    \n",
    "    def __add__(self, obj2:object):\n",
    "        \"\"\"Fuse two countor objects ('bubbles'). Better do this if they \n",
    "        intersect or one is enclosed inside the other.\"\"\"\n",
    "        \n",
    "        c1, c2 = self.contour, obj2.contour\n",
    "        fused = cv2.convexHull(np.vstack([c1, c2]))\n",
    "        new_obj = self.__class__(fused, self.lons, self.lats)\n",
    "        return new_obj\n",
    "\n",
    "\n",
    "class ContourCollection(list):\n",
    "    \"\"\"Essentially just a list, except overloads behavior for \"in\" operator.\"\"\"\n",
    "    def __init__(self, items:List[Contour]):\n",
    "        self.items = items\n",
    "        super(ContourCollection, self).__init__(items)\n",
    "        \n",
    "    def __contains__(self, x):\n",
    "        result = False\n",
    "        for c in self.items:\n",
    "            if x.name==c.name and x.area==c.area:\n",
    "                result = True\n",
    "        return result\n",
    "    \n",
    "    \n",
    "####################################\n",
    "# Find the independent contours for a given day \n",
    "####################################\n",
    "\n",
    "def find_daily_contours(ds:xr.Dataset)->List[ContourCollection]:\n",
    "    \"\"\"Give a dataset and it will loop through days and\n",
    "    find all contours per day, if any. This function does ~\n",
    "    df['contours'].rolling(window=4).sum() \"\"\"\n",
    "\n",
    "    def find_contours(arr2d: np.array, \n",
    "                      convex:bool=False, \n",
    "                      min_area:int=150) -> List[np.array]:\n",
    "        \"\"\"Encapsulate islands of 1s and return contours, [(i,j),(..),].\n",
    "        input:  day-slice of a dataset tasmax dataarray\n",
    "        output: list of contours (np.arrays)\"\"\"\n",
    "\n",
    "        H = arr2d.astype(np.uint8)\n",
    "        ret, thresh = cv2.threshold(H, 0, 1, 0, cv2.THRESH_BINARY)\n",
    "\n",
    "        kernel = np.ones((10,10), np.uint8)\n",
    "        thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "        contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        if convex:\n",
    "            contours = [cv2.convexHull(c) for c in contours]\n",
    "\n",
    "        contours = [c for c in contours if c.shape[0]>1] # filter single points\n",
    "        for c in contours:\n",
    "            if c.ndim!=3:\n",
    "                print(c.shape)\n",
    "                \n",
    "        lons = ds.coords['lon']\n",
    "        lats = ds.coords['lat']\n",
    "\n",
    "        contours = [Contour(c, lons, lats) for c in contours]\n",
    "        contours = [c for c in contours if c.area>min_area]\n",
    "\n",
    "        return ContourCollection(contours)\n",
    "\n",
    "    all_contours = []\n",
    "\n",
    "    dr = pd.DatetimeIndex(ds['time'].dt.floor('D').values.astype('str'))\n",
    "    \n",
    "    days = []\n",
    "    for d in dr:\n",
    "        day = d.strftime(\"%Y-%m-%d\")\n",
    "        extreme = ds['extreme_yn'].sel(time=day)\n",
    "        arr2d = extreme.values[0]\n",
    "\n",
    "        all_contours += [find_contours(arr2d, convex=CONVEX, min_area=MIN_AREA)]\n",
    "        days += [day]\n",
    "        \n",
    "    return all_contours, days\n",
    "\n",
    "\n",
    "####################################\n",
    "# Rolling-window contours summation on time axis\n",
    "####################################\n",
    "\n",
    "def collapse(contours:List[Contour]) -> List[Contour]:\n",
    "    \"\"\"Recursive func to fuse multiple contour objects, if overlapping.\"\"\"\n",
    "    \n",
    "    if type(contours)==float and pd.isna(contours):\n",
    "        return []\n",
    "    \n",
    "    conts = contours[:] # prevent mutation\n",
    "    for cnt1, cnt2 in itertools.combinations(conts, 2):\n",
    "        if cnt1.position_to(cnt2) in (\"inside\", \"intersect\"):\n",
    "            cnt_new = cnt1+cnt2\n",
    "            conts.remove(cnt1)\n",
    "            conts.remove(cnt2)\n",
    "            conts.append(cnt_new)\n",
    "            return collapse(conts) # recursion\n",
    "        \n",
    "    return conts\n",
    "\n",
    "\n",
    "def rolling_sum(all_contours:list, window:int=ROLLING)->pd.DataFrame:\n",
    "    \"\"\"Provide df with daily contours calculated, and it will df.rolling(w).sum()\n",
    "    The only reason we can't use pandas is that its .rolling method refuses sum(lists).\"\"\"\n",
    "    if window==1:\n",
    "        warnings.warn(\"window=1 just returns contours as-is.\")\n",
    "\n",
    "    df = pd.DataFrame(dict(contours=all_contours))\n",
    "    \n",
    "    for i in range(1, window):\n",
    "        df[f\"shift{i}\"] = df['contours'].shift(i)\n",
    "\n",
    "    df['rolling_append'] = df.filter(regex=r'contours|shift*', axis=1).dropna().sum(axis=1)\n",
    "    df['rolling_sum'] = df['rolling_append'].apply(collapse)\n",
    "\n",
    "    # drop tmp columns:\n",
    "    df = df[[c for c in df.columns if \"shift\" not in c]]\n",
    "    df = df.drop(\"rolling_append\", axis=1)\n",
    "    \n",
    "    assert len(ds['extreme_yn'])==len(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "####################################\n",
    "# Serialize metadata ready to json\n",
    "####################################\n",
    "\n",
    "def serialize(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    df1 = df.explode('contours')[['days','contours']].reset_index(drop=True)\n",
    "    df1['type'] = 'daily'\n",
    "    df1 = df1.rename({'contours':'contour'}, axis=1)\n",
    "\n",
    "    df2 = df.explode('rolling_sum')[['days','rolling_sum']].reset_index(drop=True)\n",
    "    df2['type'] = 'rolling_sum'\n",
    "    df2 = df2.rename({'rolling_sum':'contour'}, axis=1)\n",
    "\n",
    "    df3 = pd.concat([df1,df2], axis=0)\\\n",
    "                .sort_values(by=['days','type'], ascending=True)\\\n",
    "                .dropna()\\\n",
    "                .reset_index(drop=True)\n",
    "\n",
    "    df3['name'] = [x.name for x in df3['contour']]\n",
    "    df3['center'] = [x.center for x in df3['contour']]\n",
    "    df3['area'] = [x.area for x in df3['contour']]\n",
    "    df3['projected'] = [x.projected for x in df3['contour']]\n",
    "    df3 = df3.drop('contour', axis=1)\n",
    "    \n",
    "    return df3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92995689-ab78-42ff-b670-47518cda3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Generate figures for each day with contours\n",
    "####################################\n",
    "\n",
    "def validate(df:pd.DataFrame):\n",
    "    assert \"contours\" in df.columns\n",
    "    assert \"rolling_sum\" in df.columns\n",
    "    assert df.index.is_monotonic\n",
    "    \n",
    "def create_figures(df:pd.DataFrame, window:int, save=False, folder:str=None):  \n",
    "    \n",
    "    validate(df)\n",
    "    \n",
    "    def add_patches(column:str, _idx:int, color:str, linewidths:int, alpha=1):\n",
    "        contours = df[column][df.index==_idx].values[0]\n",
    "        patches = [Polygon(c.projected.squeeze(), True) for c in contours]\n",
    "\n",
    "        args = dict(edgecolors=(color,), linewidths=(linewidths,), facecolor=\"none\", alpha=alpha)\n",
    "        \n",
    "        p = PatchCollection(patches, **args)\n",
    "        ax1.add_collection(p)\n",
    "        [ax1.scatter(x=c.center[0], y=c.center[1], c=color, s=3) for c in contours]\n",
    "        \n",
    "        p = PatchCollection(patches, **args)\n",
    "        ax2.add_collection(p)\n",
    "        [ax2.scatter(x=c.center[0], y=c.center[1], c=color, s=3) for c in contours]\n",
    "    \n",
    "    for i, idx in enumerate(df.index):\n",
    "\n",
    "        dr = pd.DatetimeIndex(ds['time'].dt.floor('D').values.astype('str'))\n",
    "        day = dr[idx].strftime(\"%Y-%m-%d\")\n",
    "        tasmax = ds['tasmax'].sel(time=day)\n",
    "        tdiff = ds['above_threshold'].sel(time=day)\n",
    "        extreme = ds['extreme_yn'].sel(time=day)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2, figsize=(24,8))\n",
    "\n",
    "        im1 = extreme.squeeze().plot.imshow(ax=ax1, cmap='cividis')\n",
    "        im2 = tdiff.squeeze().plot.imshow(ax=ax2, cmap='coolwarm', vmin=4, vmax=-4, alpha=0.8)\n",
    "\n",
    "        colors = 'r b c w m g y'.split()*100\n",
    "        for x in range(i+1):\n",
    "            add_patches('contours', idx-x, colors[i-x], 1.5)\n",
    "            if x==window:\n",
    "                add_patches('rolling_sum', idx, 'g', 4, alpha=0.8) \n",
    "                break\n",
    "\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        if save:\n",
    "            # save image locally\n",
    "            if not os.path.exists(folder):\n",
    "                os.mkdir(folder)\n",
    "            fig.savefig(f\"{folder}/{day}.jpg\")\n",
    "            fig.clear()\n",
    "            plt.close(fig)\n",
    "            \n",
    "####################################\n",
    "# Compile a video from images\n",
    "####################################\n",
    "\n",
    "def create_video(files:List[str], fn_out:str)->None:\n",
    "    \n",
    "    h,w,_ = cv2.imread(files[0]).shape\n",
    "\n",
    "    with tempfile.TemporaryFile() as f:\n",
    "\n",
    "        fp = f\"{f.name}.avi\"\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        video = cv2.VideoWriter(fp, fourcc, 10, (w,h))\n",
    "\n",
    "        for fn in files:\n",
    "            img = cv2.imread(fn)\n",
    "            video.write(img)\n",
    "\n",
    "        video.release()\n",
    "        os.rename(fp, 'out.avi')\n",
    "        time.sleep(2)\n",
    "\n",
    "    fn_in = 'out.avi'\n",
    "    cmd = f\"ffmpeg -i '{fn_in}' -ac 2 -b:v 2000k -c:a aac -c:v libx264 -b:a 160k -vprofile high -bf 0 -strict experimental -f mp4 '{fn_out}'\"\n",
    "    subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)  \n",
    "    os.remove('out.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b486ac-b8f4-4e04-9447-5c7b6ac4dfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50804bb3-186d-478d-8f53-05deba78cb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7804/447345559.py:41: UserWarning: GISS_E2_1_G_ssp585_2026.json exists. Overwriting..\n",
      "  warnings.warn(f\"{self.filename} exists. Overwriting..\")\n",
      "/tmp/ipykernel_7804/447345559.py:41: UserWarning: GISS_E2_1_G_ssp585_2026.mp4 exists. Overwriting..\n",
      "  warnings.warn(f\"{self.filename} exists. Overwriting..\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GISS_E2_1_G_ssp585\t2026\t14.63 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7804/447345559.py:41: UserWarning: GISS_E2_1_G_ssp585_2027.json exists. Overwriting..\n",
      "  warnings.warn(f\"{self.filename} exists. Overwriting..\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GISS_E2_1_G_ssp585\t2027\t15.72 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7804/447345559.py:41: UserWarning: GISS_E2_1_G_ssp585_2028.json exists. Overwriting..\n",
      "  warnings.warn(f\"{self.filename} exists. Overwriting..\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GISS_E2_1_G_ssp585\t2028\t14.32 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7804/447345559.py:41: UserWarning: GISS_E2_1_G_ssp585_2029.json exists. Overwriting..\n",
      "  warnings.warn(f\"{self.filename} exists. Overwriting..\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GISS_E2_1_G_ssp585\t2029\t16.01 min\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# Run the Pipeline\n",
    "####################################\n",
    "\n",
    "models =  [\"GISS_E2_1_G_ssp585\"]  # \"GFDL_ESM4_ssp245\", \"GFDL_ESM4_ssp585\", \"GISS_E2_1_G_ssp245\"\n",
    "years = list(range(2026,2030)) \n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    for year in years:\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        # import dataset\n",
    "        ################################\n",
    "        at = AzureSource(model, year)\n",
    "        at.download()\n",
    "        ds = xr.open_mfdataset(at.filename)\n",
    "        days_above = ds.attrs['Number of continuous days to be considered extreme']\n",
    "        kelv_above = ds.attrs['threshold']\n",
    "        upload_folder = f\"NEWcontours_{days_above}days_{kelv_above}K/{model}\"\n",
    "\n",
    "        # find contours\n",
    "        ################################\n",
    "        dc, days = find_daily_contours(ds)\n",
    "        df_daily = rolling_sum(dc)\n",
    "        df_daily['days'] = days\n",
    "\n",
    "        # create metadata\n",
    "        ################################\n",
    "        path_meta = f\"{model}_{year}.json\"\n",
    "        df_meta = serialize(df_daily)\n",
    "        df_meta.to_json(path_meta)\n",
    "        \n",
    "        # create images\n",
    "        ################################\n",
    "        img_folder = f\"{model}_{year}\"\n",
    "        create_figures(df_daily, window=ROLLING, save=True, folder=img_folder)\n",
    "        figs = sorted([str(p) for p in Path(img_folder).rglob(\"*.jpg\")])\n",
    "        \n",
    "        # create video\n",
    "        ################################\n",
    "        path_video = f\"{model}_{year}.mp4\"\n",
    "        create_video(figs, path_video)\n",
    "        \n",
    "        # export all to Azure\n",
    "        ################################\n",
    "        AzureTarget(path_meta).upload(upload_folder)\n",
    "        [AzureTarget(fn).upload(upload_folder) for fn in figs]\n",
    "        AzureTarget(path_video).upload(upload_folder)\n",
    "        \n",
    "        # delete local files\n",
    "        ################################\n",
    "        os.remove(path_meta)\n",
    "        shutil.rmtree(img_folder) \n",
    "        os.remove(path_video)\n",
    "        os.remove(at.filename)\n",
    "        \n",
    "        print(f\"{model}\\t{year}\\t{round((time.time()-t1)/60,2)} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b63bd44-f81e-4337-8387-74d67f72dc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29a81f-b98a-4232-a57c-0af6158e21fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15717b1-ae0b-46e3-aa1d-b579dc26aa62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
